{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e2b7bb-ee73-47b0-9674-40db5aac0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 40\n",
    "LABEL_COUNTS = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fb7a523-0fde-4136-9afa-b6e53fca0fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available, \"GPU が利用可能ではない。\"\n",
    "\n",
    "device = torch.device(\"cuda\", index=1)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da7b3bb-22fb-4993-9ce6-70ab554ae87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, logging\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "encoder = AutoModel.from_pretrained(model_name)\n",
    "logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3693330c-145d-4607-bc39-6ff3367b5bae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5552e738-79a4-4589-ada6-b0ec6c63900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from nltk import Tree\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class TreebankDataset(Dataset):\n",
    "    def __init__(self, tokenizer, tree_path, gold_path):\n",
    "        self.tokenizer = tokenizer\n",
    "        trees = [Tree.fromstring(tree) for tree in Path(tree_path).read_text().splitlines()]\n",
    "        self.trees, token_ids = zip(*self.encode(trees))\n",
    "        self.token_ids = torch.as_tensor(token_ids).to(device)\n",
    "        self.gold_labels = torch.load(gold_path).to(device)\n",
    "\n",
    "        self.token_ids.requires_grad = False\n",
    "        self.gold_labels.requires_grad = False\n",
    "\n",
    "    # メソッド名おかしい\n",
    "    def encode(self, trees, max_len=MAX_LEN):\n",
    "        for tree in tqdm(trees):\n",
    "            words = tree.leaves()\n",
    "            token_ids = []\n",
    "            for word in words:\n",
    "                first_token_id, *_ = self.tokenizer.encode(\n",
    "                    word, add_special_tokens=False)\n",
    "                token_ids.append(first_token_id)\n",
    "\n",
    "            # TODO: add attention mask\n",
    "            padding = [self.tokenizer.pad_token_id] * (max_len - len(token_ids))\n",
    "            token_ids = [self.tokenizer.cls_token_id] + token_ids + padding + [self.tokenizer.sep_token_id]\n",
    "            \n",
    "            yield tree, token_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.trees)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"return batch\n",
    "        \n",
    "        token_ids, gold_labels\n",
    "        \"\"\"\n",
    "        return self.token_ids[index], self.gold_labels[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bdaa95-6749-417a-a801-f3da0f1198b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2a21a18-05b7-4d5c-a2a8-2aae21d26593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Parser(nn.Module):\n",
    "    def __init__(self, encoder, label_counts=LABEL_COUNTS):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        # TODO: 学習済みモデルの読み込み\n",
    "        # if True or fine_tuned_model:\n",
    "        #     config = AutoConfig(base_model)\n",
    "        #     self.bert = AutoModel.from_config(config)\n",
    "        #     self.load_state_dict(torch.load(fine_tuned_model))           \n",
    "        # else:\n",
    "        #     self.bert = AutoModel.from_pretrained(base_model)\n",
    "\n",
    "        # 768 -> 300 -> 60\n",
    "        hidden_dim = 300\n",
    "        self.label_counts = label_counts\n",
    "        self.mlp = nn.Sequential(nn.Linear(self.encoder.config.hidden_size,\n",
    "                                           hidden_dim, bias=False),\n",
    "                                 nn.LayerNorm(hidden_dim),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(hidden_dim, label_counts, bias=False)\n",
    "                                 )\n",
    "\n",
    "    # 簡単な配列でテストする\n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"calculate scores for each label.\n",
    "        \n",
    "        index: [820, 2]\n",
    "        labels: [25, 820]\n",
    "        max_scores: [25, 820]\n",
    "        all_scores: [25, 820, 60]\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # dataset ですべきでは？\n",
    "            embed = self.encoder(token_ids, return_dict=True)[\"last_hidden_state\"]\n",
    "\n",
    "            batch_size, sent_len, embed_dim = embed.shape\n",
    "            y_left = embed[:, 1:, embed_dim//2:]\n",
    "            y_right = embed[:, :-1, :embed_dim//2]\n",
    "            fence = torch.cat([y_right, y_left], dim=2)\n",
    "\n",
    "        # 文の長さに合わせて学習したい\n",
    "        indexes = list(combinations(range(sent_len-1), 2))\n",
    "        all_scores = torch.empty(batch_size, len(indexes), self.label_counts).to(device)\n",
    "        for i, (start, end) in enumerate(indexes):\n",
    "            span = fence[:, end, :] - fence[:, start, :]\n",
    "            all_scores[:, i, :] = self.mlp(span)\n",
    "\n",
    "        max_score = torch.max(all_scores, dim=2)\n",
    "        labels = max_score.indices\n",
    "        max_scores = max_score.values\n",
    "\n",
    "        labels.requires_grad = False\n",
    "\n",
    "        return indexes, labels, max_scores, all_scores\n",
    "\n",
    "    def parse(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eaa435-ecd1-4c34-a4ee-048fe1677d5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82fdbaf1-19e1-41e9-b364-f5211ddef39b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import cache\n",
    "\n",
    "\n",
    "def convert_index(i, j, n):\n",
    "    \"\"\"tuple to combination's index\"\"\"\n",
    "    assert 0 <= i < j <= n\n",
    "    return int(i * (n + (n - i - 1)) / 2 + j - 1)\n",
    "\n",
    "\n",
    "def score_gold_tree(label_scores, max_len=MAX_LEN):\n",
    "    # 後で gold に最適化する\n",
    "    @cache\n",
    "    def score_subtree(i, j):\n",
    "        label_max = label_scores[:, convert_index(i, j, max_len)]\n",
    "        split_max = torch.zeros_like(label_max, device=device)\n",
    "        for k in range(i+1, j):\n",
    "            tmp_max = score_subtree(i, k) + score_subtree(k, j)\n",
    "            split_max = torch.maximum(split_max, tmp_max)\n",
    "\n",
    "        return label_max + split_max\n",
    "    return score_subtree(0, max_len)\n",
    "\n",
    "\n",
    "def score_pred_tree(label_scores, _pred_labels, _gold_labels, max_len=MAX_LEN):\n",
    "    @cache\n",
    "    def score_subtree(i, j):\n",
    "        single_index = convert_index(i, j, max_len)\n",
    "        label_max = label_scores[:, single_index]\n",
    "        split_max = torch.zeros_like(label_max, device=device)\n",
    "        for k in range(i+1, j):\n",
    "            left_score, left_label = score_subtree(i, k)\n",
    "            right_score, right_label = score_subtree(k, j)\n",
    "\n",
    "            new_max = split_max < left_score + right_score\n",
    "            \n",
    "            \n",
    "            split_max = torch.maximum(split_max, left_score + right_score)\n",
    "\n",
    "        # これだとルートノードの margin しか返してない？\n",
    "        gold_labels = _gold_labels[:, single_index]\n",
    "        pred_labels = _pred_labels[:, single_index]\n",
    "        # 教師データにスパンが存在しない範囲は無視\n",
    "        margin = torch.where(gold_labels == -1, 0, (pred_labels != gold_labels).to(int))\n",
    "        return label_max + split_max + margin, pred_labels\n",
    "    return score_subtree(0, max_len)\n",
    "\n",
    "\n",
    "def hamming(pred_labels, gold_labels):\n",
    "    return torch.sum(pred_labels != gold_labels)\n",
    "\n",
    "\n",
    "def loss_fn(max_scores, all_scores, pred_labels, gold_labels,\n",
    "            zero=torch.tensor(0., requires_grad=False).to(device)):\n",
    "    batch_size, span_counts, label_count = all_scores.shape\n",
    "    gold_scores = torch.stack([all_scores[i, range(span_counts), gold_labels[i]]\n",
    "                               for i in range(batch_size)]).detach()\n",
    "    gold_scores = gold_scores.where(gold_labels == -1, zero)\n",
    "\n",
    "    gold_tree_score = score_gold_tree(gold_scores)\n",
    "    pred_tree_score = score_pred_tree(max_scores, pred_labels, gold_labels)\n",
    "    loss = torch.maximum(zero, torch.sum(pred_tree_score - gold_tree_score))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf1ff1-76e6-4bfb-8502-1382ed292edf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e302a51-92a3-4c34-943e-315944d4d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "parser = Parser(encoder).to(device)\n",
    "optimizer = AdamW([{\"params\": parser.parameters()}], lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dda191f-3a14-41db-81fb-dd20a9852e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1525/1525 [00:01<00:00, 1108.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def make_loader(phase, batch_size=50):\n",
    "    dataset = TreebankDataset(tokenizer, f\"data/{phase}\", f\"data/{phase}_chart.pt\")\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# train = make_loader(\"train\", 500)\n",
    "valid = make_loader(\"valid\", 500)\n",
    "# test = make_loader(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e223ae2-4f97-4a19-93ba-a22bf260dcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:10<00:00,  2.71s/it]\n",
      "100%|██████████| 4/4 [00:11<00:00,  2.80s/it]\n",
      "  0%|          | 0/4 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m indexes, pred_labels, max_scores, all_scores \u001b[38;5;241m=\u001b[39m parser(token_ids)\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(max_scores, all_scores, pred_labels, gold_labels)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/envs/python3.9/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/python3.9/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(\"log.txt\", \"w\") as f:\n",
    "    print(end=\"\", file=f)\n",
    "\n",
    "torch.cuda.manual_seed_all(0)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    loss_sum = 0\n",
    "    for token_ids, gold_labels in tqdm(valid):\n",
    "        indexes, pred_labels, max_scores, all_scores = parser(token_ids)\n",
    "        loss = loss_fn(max_scores, all_scores, pred_labels, gold_labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_sum += loss\n",
    "        \n",
    "        # tqdm.write(repr(margin))\n",
    "\n",
    "        # del loss\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "    with open(\"log.txt\", \"a\") as f:\n",
    "        print(f\"epoch: {epoch:2}, loss: {loss_sum:9.2f}, \"\n",
    "              f\"ave. loss: {loss_sum/len(valid.dataset):5.3f}\",\n",
    "              file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab011d0f-a5c3-44b4-9ba9-0d0091fa5677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
